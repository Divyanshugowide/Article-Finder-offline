

# ğŸ§  Article Finder â€” Offline-First

A lightweight, **offline-capable document search engine** that retrieves English excerpts with citations and supports **role-based access control (RBAC)**.
Built for environments with limited or no internet access.

---

## ğŸš€ Quick Start

### **1ï¸âƒ£ Environment Setup**

Create and activate a Python virtual environment:

```bash
python -m venv .venv
# macOS / Linux
source .venv/bin/activate
# Windows PowerShell
.\.venv\Scripts\Activate.ps1
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

### **2ï¸âƒ£ Add Your PDFs**

Place all PDF files inside:

```
data/raw_pdfs/
```

> âš ï¸ Do **not** commit PDFs to version control (they may contain sensitive or large data).

---

### **3ï¸âƒ£ Build the Processing Pipeline**

Run the following scripts in sequence:

```bash
python scripts/01_chunk_pdfs.py
python scripts/02_build_bm25.py
python scripts/03_build_faiss.py
```

After running successfully, you should see:

```
âœ… data/processed/chunks.jsonl
âœ… data/idx/bm25.pkl
âœ… data/idx/mE5.faiss
âœ… data/idx/meta.json
```

---

### **4ï¸âƒ£ Start the API Server**

Run:

```bash
uvicorn app.run_api:app --host 0.0.0.0 --port 8000
```

---

### **5ï¸âƒ£ Example Query**

Use `curl` to send a query:

```bash
curl -X POST "http://127.0.0.1:8000/ask" -H "Content-Type: application/json" \
  -d '{"user_id":"demo","roles":["staff"],"query":"What is the operator\'s liability limit?"}'
```

Example response:

```json
{
  "answer": "The operatorâ€™s liability limit is defined under Section 10.2.3...",
  "results": [
    {
      "doc_id": "Transport_Regulations.pdf",
      "page_start": 85,
      "page_end": 87,
      "roles": ["staff", "legal"]
    }
  ]
}
```

---

## ğŸ§© Offline Model Setup

By default, this project uses the model
`senteÂ­nce-transformers/all-MiniLM-L6-v2`
for FAISS embeddings.

To ensure full **offline functionality**, use one of the following methods:

---

### **Option 1 â€” Cache the Model (simplest)**

Before going offline for the first time, cache the model locally:

```bash
# Linux / macOS
export TRANSFORMERS_CACHE=$(pwd)/models_cache
mkdir -p models_cache

# Windows PowerShell
set TRANSFORMERS_CACHE=./models_cache
mkdir models_cache
```

Then build FAISS once:

```bash
python scripts/03_build_faiss.py
```

âœ… The model will now be stored in `./models_cache`
and automatically reused offline later.

---

### **Option 2 â€” Fully Local Model (no internet ever needed)**

1. **Download the model manually** (on any machine with internet):
   ğŸ”— [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)

   Use either:

   ```bash
   git lfs install
   git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
   ```

   or click **Download repository** on the Hugging Face page.

2. **Move it to your offline environment:**

   ```
   models/all-MiniLM-L6-v2/
   ```

3. **Update your code** (in `scripts/03_build_faiss.py`):

   ```python
   from sentence_transformers import SentenceTransformer
   import os

   LOCAL_MODEL_PATH = os.getenv("MODEL_NAME", "models/all-MiniLM-L6-v2")
   model = SentenceTransformer(LOCAL_MODEL_PATH)
   ```

âœ… Now, FAISS will build embeddings using your local model â€” no internet connection required.

---

### **Option 3 â€” Use BM25 Only (no embeddings)**

If you prefer a completely neural-free setup:

```bash
python scripts/01_chunk_pdfs.py
python scripts/02_build_bm25.py
```

This uses classic keyword-based BM25 ranking â€” 100% offline.
(FAISS step can be skipped.)

---

## ğŸ§± Project Structure

```
article-finder/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ run_api.py          # FastAPI server
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdfs/           # Input PDFs
â”‚   â”œâ”€â”€ processed/          # Chunks and normalized text
â”‚   â””â”€â”€ idx/                # BM25 + FAISS indices
â”œâ”€â”€ models/
â”‚   â””â”€â”€ all-MiniLM-L6-v2/   # Local model for offline use
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_chunk_pdfs.py
â”‚   â”œâ”€â”€ 02_build_bm25.py
â”‚   â”œâ”€â”€ 03_build_faiss.py
â”‚   â”œâ”€â”€ 04_query_cli.py
â”‚   â””â”€â”€ 05_api.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Self-Test Checklist

| Step        | Command                                                      | Expected Output                   |
| ----------- | ------------------------------------------------------------ | --------------------------------- |
| Chunk PDFs  | `python scripts/01_chunk_pdfs.py`                            | `data/processed/chunks.jsonl`     |
| Build BM25  | `python scripts/02_build_bm25.py`                            | `data/idx/bm25.pkl`               |
| Build FAISS | `python scripts/03_build_faiss.py`                           | `data/idx/mE5.faiss`, `meta.json` |
| CLI Query   | `python scripts/04_query_cli.py --query "..." --roles staff` | Answer + Chunks                   |
| API Test    | `curl http://127.0.0.1:8000/health`                          | `{"ok": true}`                    |

---

## ğŸ”’ RBAC (Role-Based Access Control)

1. Rename a file to include `"restricted"` (e.g., `Contract_restricted.pdf`)
2. Rebuild indices
3. Query as:

   ```json
   {"roles": ["staff"]}
   ```

   â†’ restricted doc **hidden**
4. Query as:

   ```json
   {"roles": ["legal"]}
   ```

   â†’ restricted doc **visible**

---
 ğŸ’¡ Tip

Once downloaded, **zip the `models/all-MiniLM-L6-v2/` folder**
and reuse it for all future offline deployments â€” no re-downloads needed.

---

### âœ… Example End-to-End (Offline Setup)

```bash
# 1. Activate environment
.\.venv\Scripts\Activate.ps1

# 2. Place PDFs in data/raw_pdfs/
# 3. Ensure model is saved locally in models/all-MiniLM-L6-v2/

# 4. Build pipeline fully offline
python scripts/01_chunk_pdfs.py
python scripts/02_build_bm25.py
python scripts/03_build_faiss.py

# 5. Launch API
uvicorn app.run_api:app --host 0.0.0.0 --port 8000
```

Now you have a **completely offline, role-aware article finder** running locally ğŸ¯
